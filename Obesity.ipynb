{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71388c52",
   "metadata": {},
   "source": [
    "# Obesity Level Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e95033",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcdcda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pickle\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2c3d02",
   "metadata": {},
   "source": [
    "- Load all the necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecec5e0",
   "metadata": {},
   "source": [
    "## Understanding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec75415",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('ObesityDataSet1.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee4ff12",
   "metadata": {},
   "source": [
    "- Load the dataset and display 5 sample rows to get an overview of the data  \n",
    "- I also displayed the shape of the dataset to see the number of rows and columns\n",
    "- The dataset consists of 1,055 rows and 17 columns\n",
    "- At this point, an anomaly can already be seen in the \"Age\" column due to inconsistencies in how the ages are written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeb0171",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22b7bb8",
   "metadata": {},
   "source": [
    "- Due to the inconsistency in formatting, the \"Age\" column is recognized as an object data type, whereas it should be of integer type  \n",
    "- At this stage, we can also observe the presence of null values in the \"CAEC\" and \"TUE\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a90ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.rename(columns= {\"family_history_with_overweight\": \"FamilyHistory\" })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae97ab43",
   "metadata": {},
   "source": [
    "- I renamed \"family_history_with_overweight\" column to make it easier to type, as the original names were too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12d2ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fad0545",
   "metadata": {},
   "source": [
    "- Now the column names are much more convenient to work with, as there are no overly long column names anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1028e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.NObeyesdad.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15d9957",
   "metadata": {},
   "source": [
    "- The \"NObeyesdad\" column is the target variable. Its class distribution is relatively balanced across all categories, so there is no need to apply oversampling techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4719ca",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6a51b2",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7093ef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= df[df.columns.drop(['NObeyesdad'])]\n",
    "y= df['NObeyesdad']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7846e1e6",
   "metadata": {},
   "source": [
    "- Separate the target column (y) from the feature columns (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca703a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.2, random_state= 42, stratify= y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2194e2fa",
   "metadata": {},
   "source": [
    "- The data is split into training and testing sets with an 80:20 ratio. This split allows the model to learn from the training data and be evaluated on unseen data\n",
    "- The `stratify=y` parameter is used during the split to ensure that the distribution of the target classes remains consistent in both training and testing sets, even though the original class distribution is already fairly balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f09f1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distribution of Obesity Level in Data Train:\")\n",
    "print(pd.Series(y_train).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8bd5c1",
   "metadata": {},
   "source": [
    "- The training set remains balanced due to the use of `stratify= y` during the split, and because the original dataset was already fairly well distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba11baaa",
   "metadata": {},
   "source": [
    "### Check Distribution in Every Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dbf77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train['Gender'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f4335b",
   "metadata": {},
   "source": [
    "- The \"Gender\" column in the training set is relatively balanced, with 432 males and 412 females.  \n",
    "- This balance helps prevent gender related bias during model training and ensures the model learns patterns that are not skewed toward a particular gender group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b2911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train['Age'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5340e5",
   "metadata": {},
   "source": [
    "- An anomaly is found in the \"Age\" column where some entries include the string \"years\" (e.g., \"44 years\") instead of just numeric values. This inconsistency causes the column to be treated as an object type instead of numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9483a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train['Height'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd13b63",
   "metadata": {},
   "source": [
    "- There is a wide range of values in the \"Height\" column, but the most frequently occurring height is 170 cm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b66def",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train['Weight'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b37c7a4",
   "metadata": {},
   "source": [
    "- Similar to height, the \"Weight\" column has diverse values, with 75 kg and 70 kg being the most frequently occurring weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a2c3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train['FamilyHistory'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7ab6aa",
   "metadata": {},
   "source": [
    "- Most entries in the \"FamilyHistory\" column indicate a family history of obesity, with \"yes\" being the dominant response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22f01a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train['FAVC'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2044c6",
   "metadata": {},
   "source": [
    "- The \"FAVC\" column indicates whether a person frequently consumes high calorie food. Most respondents answered \"yes\", suggesting that high calorie food consumption is common among the individuals in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc7239",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train['FCVC'].value_counts(), '\\n')\n",
    "print(x_train[~x_train['FCVC'].between(1, 3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb71e534",
   "metadata": {},
   "source": [
    "- The \"FCVC\" column represents the frequency of vegetable consumption on a scale from 1 to 3. Most individuals report a frequency of 2.0 or 3.0, indicating that many people in the dataset regularly consume vegetables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0db6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train['NCP'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed45ac32",
   "metadata": {},
   "source": [
    "- The \"NCP\" column represents the number of main meals per day. The majority of individuals report having 3 meals a day, which aligns with common eating patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06606bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train['CAEC'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833aa6b8",
   "metadata": {},
   "source": [
    "- The \"CAEC\" column shows how frequently individuals consume food between meals. Most responses fall under \"Sometimes\", followed by \"Frequently\", while fewer individuals answered \"Always\" or \"no\" \n",
    "- This suggests that snacking between meals is a common habit in the dataset, which could potentially contribute to higher calorie intake and weight gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a245d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train['SMOKE'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5310cd86",
   "metadata": {},
   "source": [
    "- The \"SMOKE\" column indicates whether individuals smoke. The vast majority answered \"no\", while only a small number reported smoking\n",
    "- This suggests that smoking is not a common habit among the participants in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b378e20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train['CH2O'].value_counts(), '\\n')\n",
    "print(x_train[~x_train['CH2O'].between(1, 3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafab96a",
   "metadata": {},
   "source": [
    "- The \"CH2O\" column represents daily water intake on a scale from 1 to 3. Most individuals reported a value of 2.0, followed by 1.0 and 3.0.  \n",
    "- This indicates that the majority of people in the dataset consume a moderate amount of water daily, though there is some variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9acfe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train['SCC'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149b7d62",
   "metadata": {},
   "source": [
    "- The \"SCC\" column indicates whether individuals monitor their calorie intake. Most respondents answered \"no\", with only a small portion answering \"yes\"  \n",
    "- This suggests that calorie tracking is not a common practice among the participants, which may affect their awareness of daily nutritional intake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff9045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train['FAF'].value_counts(), '\\n')\n",
    "print(x_train[~x_train['FAF'].between(0, 3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125c0959",
   "metadata": {},
   "source": [
    "- The \"FAF\" column represents the frequency of physical activity on a scale from 0 to 3. A significant number of individuals reported 0.0, indicating no physical activity, while others showed varying levels of activity\n",
    "- This suggests that a notable portion of the dataset leads a sedentary lifestyle, which could contribute to higher obesity risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e8d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train['TUE'].value_counts(), '\\n')\n",
    "print(x_train[~x_train['TUE'].between(0, 3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497f56ad",
   "metadata": {},
   "source": [
    "- The \"TUE\" column represents time spent using technology on a scale from 0 to 3. Most values fall within this expected range, with 0.0 being the most frequent  \n",
    "- The code `x_train[~x_train['TUE'].between(0, 3)]` is used to check for anomalies specifically values that fall outside the valid range of 0 to 3\n",
    "- The resulting output includes some rows due to the presence of null values (missing data), not because the values are truly out of range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a54327",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train['CALC'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51de824",
   "metadata": {},
   "source": [
    "- The \"CALC\" column indicates the frequency of alcohol consumption, with categories such as \"no\", \"Sometimes\", \"Frequently\", and \"Always\"\n",
    "- Most individuals selected \"Sometimes\", followed by \"no\", suggesting that occasional alcohol consumption is relatively common in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95241163",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train['MTRANS'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f750441",
   "metadata": {},
   "source": [
    "- The \"MTRANS\" column represents the main mode of transportation used by individuals  \n",
    "- The majority rely on public transportation, followed by automobiles. Very few individuals use walking, motorbikes, or bicycles  \n",
    "- This suggests a high dependency on motorized transport, which may correlate with lower physical activity levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72afe87",
   "metadata": {},
   "source": [
    "### Anomaly Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c3f35c",
   "metadata": {},
   "source": [
    "#### Age Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70ba20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_age(age_val):\n",
    "  if isinstance(age_val, str):\n",
    "    return float(''.join(filter(str.isdigit, age_val)))\n",
    "  return age_val\n",
    "\n",
    "x_train['Age']= x_train['Age'].apply(clean_age)\n",
    "x_test['Age']= x_test['Age'].apply(clean_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d6bf4c",
   "metadata": {},
   "source": [
    "- A custom function `clean_age` is defined to clean the \"Age\" column. If the value is a string (e.g., \"44 years\"), it removes all non digit characters and converts the result to a float  \n",
    "- This is necessary because some age values were stored as strings with additional text, which caused data type issues  \n",
    "- The function is applied to both the training and testing sets to ensure that the \"Age\" column contains only numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4a0a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train['Age'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959752e2",
   "metadata": {},
   "source": [
    "- Now the \"Age\" column contains only clean, numeric values, and is correctly formatted for further analysis or modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93617c76",
   "metadata": {},
   "source": [
    "## Handling Duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2fa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5908e865",
   "metadata": {},
   "source": [
    "- There are 8 duplicated rows in the training data. These duplicates are removed to prevent bias and redundancy during model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624b2ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_shape= x_train.shape[0]\n",
    "\n",
    "train_dup_mask= x_train.duplicated(keep= 'first')\n",
    "\n",
    "x_train_clean= x_train[~train_dup_mask]\n",
    "y_train_clean= y_train[~train_dup_mask]\n",
    "\n",
    "print(f\"Training set: Removed {original_train_shape - x_train_clean.shape[0]} duplicates\")\n",
    "\n",
    "x_train, y_train= x_train_clean, y_train_clean\n",
    "\n",
    "assert len(x_train)== len(y_train), \"x-y training set misalignment!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacce318",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_test_shape= x_test.shape[0]\n",
    "\n",
    "test_dup_mask= x_test.duplicated(keep='first')\n",
    "\n",
    "x_test_clean= x_test[~test_dup_mask]\n",
    "y_test_clean= y_test[~test_dup_mask]\n",
    "\n",
    "print(f\"Test set: Removed {original_test_shape - x_test_clean.shape[0]} duplicates\")\n",
    "\n",
    "x_test, y_test= x_test_clean, y_test_clean\n",
    "\n",
    "assert len(x_test) == len(y_test), \"x-y test set misalignment!\"\n",
    "\n",
    "print(\"\\nData shapes after cleaning:\")\n",
    "print(f\"Train: x {x_train.shape}, y {y_train.shape}\")\n",
    "print(f\"Test : x {x_test.shape}, y {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b725893",
   "metadata": {},
   "source": [
    "- Duplicate data is removed from both the training and test sets  \n",
    "- In the training set, duplicates are removed to ensure the model learns from diverse and non redundant data, which helps improve generalization. \n",
    "- In the test set, duplicates are also removed to avoid biased evaluation and ensure the classification report reflects the model’s performance on truly unique samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a33ee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d494e29",
   "metadata": {},
   "source": [
    "## Null Values Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74317d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols= ['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']\n",
    "ordinal_cols= ['Gender', 'FamilyHistory', 'FAVC', 'CAEC', 'SMOKE', 'SCC', 'CALC']\n",
    "onehot_cols= ['MTRANS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0229e247",
   "metadata": {},
   "source": [
    "- The features are divided into three groups to be processed differently in a preprocessing pipeline:\n",
    "  - `numerical_cols`: Continuous numerical features that will be scaled.\n",
    "  - `ordinal_cols`: Categorical features with an implied order or limited categories, which will be encoded using ordinal encoding.\n",
    "  - `onehot_cols`: Nominal categorical features with no intrinsic order, which will be encoded using one hot encoding.\n",
    "- This separation allows each type of feature to be preprocessed appropriately within a unified pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36202834",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Null Values in Data Train')\n",
    "print(x_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990e69c3",
   "metadata": {},
   "source": [
    "- As observed earlier, the \"CAEC\" and \"TUE\" columns contain some null values. However, since the number of missing entries is small, I choose to impute them instead of dropping the columns  \n",
    "- These features are considered important (e.g., \"CAEC\" relates to snacking habits and \"TUE\" to technology usage), so retaining them with imputation helps preserve potentially valuable information for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dca565",
   "metadata": {},
   "source": [
    "#### Check Outliers in Numerical Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eb1df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in numerical_cols:\n",
    "  plt.figure(figsize= (8,5))\n",
    "  sns.boxplot(y= x_train[i])\n",
    "  plt.xlabel(i)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afbeb7e",
   "metadata": {},
   "source": [
    "- Since there are outliers in several numerical columns, median imputation is used as it is more robust to extreme values than mean imputation  \n",
    "- Additionally, the data is not normally distributed, so `RobustScaler` is applied for scaling. This scaler is less sensitive to outliers compared to other scaling methods like MinMax or StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4771b5d0",
   "metadata": {},
   "source": [
    "## Category Distributions for Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f58d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ordinal_cols:\n",
    "  print(x_train[i].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ac0369",
   "metadata": {},
   "source": [
    "- This step is used to inspect the distribution of values in each categorical column. It helps in understanding the data and deciding the appropriate order for ordinal encoding \n",
    "- By examining the unique values and their frequencies, we can determine how to handle each column during preprocessing and ensure the encoding makes logical sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bb65b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in onehot_cols:\n",
    "  print(x_train[i].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e9ca38",
   "metadata": {},
   "source": [
    "## Pipeline Handling Null Values and Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4279e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_categories= [\n",
    "  ['Female', 'Male'],\n",
    "  ['no', 'yes'],\n",
    "  ['no', 'yes'],\n",
    "  ['no', 'Sometimes', 'Frequently', 'Always'],\n",
    "  ['no', 'yes'],\n",
    "  ['no', 'yes'],\n",
    "  ['no', 'Sometimes', 'Frequently', 'Always']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909bab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline= Pipeline([\n",
    "  ('imputer', SimpleImputer(strategy= 'median')),\n",
    "  ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "ordinal_pipeline= Pipeline([\n",
    "  ('imputer', SimpleImputer(strategy= 'most_frequent')),\n",
    "  ('encoder', OrdinalEncoder(categories= ordinal_categories))\n",
    "])\n",
    "\n",
    "onehot_pipeline= Pipeline([\n",
    "  ('imputer', SimpleImputer(strategy= 'most_frequent')),\n",
    "  ('encoder', OneHotEncoder(handle_unknown= 'ignore'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96f9cd9",
   "metadata": {},
   "source": [
    "- Custom category orders are defined in `ordinal_categories` to guide the `OrdinalEncoder`, ensuring that each ordinal feature is encoded in a meaningful and logical order\n",
    "\n",
    "- Three separate pipelines are created to handle different types of features:\n",
    "\n",
    "  - `num_pipeline`: Handles numerical columns using median imputation (to deal with outliers) and `RobustScaler` (to scale values while minimizing the influence of outliers)\n",
    "  \n",
    "  - `ordinal_pipeline`: Applies to ordinal categorical features, using most frequent value imputation and ordinal encoding based on the predefined order in `ordinal_categories`\n",
    "  \n",
    "  - `onehot_pipeline`: Used for nominal categorical features, with most frequent imputation and one hot encoding. The `handle_unknown='ignore'` parameter ensures the pipeline doesn’t break when encountering unseen categories during inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9041877",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor= ColumnTransformer([\n",
    "    ('num', num_pipeline, numerical_cols),\n",
    "    ('ordinal', ordinal_pipeline, ordinal_cols),\n",
    "    ('onehot', onehot_pipeline, onehot_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73919af",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7506ce",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef54438",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_check= preprocessor.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5588e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb0d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = preprocessor.get_feature_names_out()\n",
    "x_train_check_df = pd.DataFrame(x_train_check, columns=feature_names)\n",
    "\n",
    "print(\"Shape:\", x_train_check_df.shape)\n",
    "x_train_check_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62406f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Null Values in Data Train')\n",
    "print(x_train_check_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783de4f3",
   "metadata": {},
   "source": [
    "- This step is used to verify that the preprocessing was correctly applied\n",
    "- After transforming the training data using `preprocessor.fit_transform(x_train)`, the result is a NumPy array \n",
    "- To make it more interpretable, the array is converted back into a DataFrame using the feature names from `preprocessor.get_feature_names_out()`  \n",
    "- The resulting DataFrame shows that scaling, encoding, and imputations were successfully applied to the appropriate columns as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d76a086",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62046eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline= Pipeline([\n",
    "  ('preprocessing', preprocessor),\n",
    "  ('classifier', RandomForestClassifier(\n",
    "    n_estimators= 100,\n",
    "    max_depth= 4,\n",
    "    min_samples_split= 4,\n",
    "    min_samples_leaf= 2,\n",
    "    class_weight= 'balanced',\n",
    "    random_state= 42,\n",
    "    n_jobs= -1\n",
    "  ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10700c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_pipeline= Pipeline([\n",
    "  ('preprocessing', preprocessor),\n",
    "  ('classifier', GradientBoostingClassifier(\n",
    "    n_estimators= 100,\n",
    "    learning_rate= 0.05,\n",
    "    max_depth= 4,\n",
    "    min_samples_split= 3,\n",
    "    min_samples_leaf= 2,\n",
    "    subsample= 0.8,\n",
    "    loss= 'log_loss',\n",
    "    random_state= 42\n",
    "   ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad8cd59",
   "metadata": {},
   "source": [
    "- Two different models are used for comparison: `RandomForestClassifier` and `GradientBoostingClassifier`.  \n",
    "- Both models are wrapped inside pipelines that include the preprocessing steps, ensuring consistent data transformation during training and evaluation\n",
    "- This comparison allows us to evaluate which model performs better on the given dataset and should be chosen for final deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478cedc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdcd292",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_pipeline.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ceef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict1= rf_pipeline.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59825189",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict2= gb_pipeline.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1bcc26",
   "metadata": {},
   "source": [
    "### Evaluation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f61435",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification Report Random Forest\\n')\n",
    "print(classification_report(y_test, y_predict1))\n",
    "\n",
    "labels= ['Insufficient_Weight', 'Normal_Weight', 'Obesity_Type_I','Obesity_Type_II', 'Obesity_Type_III', 'Overweight_Level_I', 'Overweight_Level_II']\n",
    "\n",
    "cm= confusion_matrix(y_test, y_predict1, labels= labels)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot= True, fmt= 'd', cmap= 'YlGnBu', xticklabels= labels, yticklabels= labels)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix - Random Forest')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac8014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification Report Gradient Boost\\n')\n",
    "print(classification_report(y_test, y_predict2))\n",
    "\n",
    "labels= ['Insufficient_Weight', 'Normal_Weight', 'Obesity_Type_I','Obesity_Type_II', 'Obesity_Type_III', 'Overweight_Level_I', 'Overweight_Level_II']\n",
    "\n",
    "cm= confusion_matrix(y_test, y_predict2, labels= labels)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot= True, fmt= 'd', cmap= 'YlGnBu', xticklabels= labels, yticklabels= labels)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix - Gradient Boost')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a51644b",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1348454",
   "metadata": {},
   "source": [
    "##### Classification Report Comparison\n",
    "| Metric         | Random Forest | Gradient Boost |\n",
    "|----------------|----------------|----------------|\n",
    "| **Accuracy**   | 0.76           | **0.94**       |\n",
    "| **Macro Avg Precision** | 0.77   | **0.94**       |\n",
    "| **Macro Avg Recall**    | 0.76   | **0.94**       |\n",
    "| **Macro Avg F1-score**  | 0.75   | **0.94**       |\n",
    "\n",
    "- **Gradient Boosting** significantly outperforms **Random Forest** in all key classification metrics  \n",
    "- It achieves higher precision, recall, and F1-score across most classes, especially evident in the macro average, which is important in multi-class problems with balanced data\n",
    "\n",
    "---\n",
    "\n",
    "##### Per Class Performance Insights\n",
    "\n",
    "- **Random Forest** shows:\n",
    "  - High recall on `Insufficient_Weight` and `Obesity_Type_II` (1.00 and 0.97), but\n",
    "  - Poor performance on `Normal_Weight` (recall = 0.48, F1 = 0.55) and `Overweight_Level_I` (F1 = 0.62).\n",
    "  - Perfect classification for `Obesity_Type_III`.\n",
    "\n",
    "- **Gradient Boost** shows:\n",
    "  - High and consistent scores across all classes (recall and precision mostly >= 0.88),\n",
    "  - Perfect classification for `Obesity_Type_III`.\n",
    "\n",
    "---\n",
    "\n",
    "##### Confusion Matrix Comparison\n",
    "\n",
    "- **Random Forest Confusion Matrix:**\n",
    "  - Misclassifies a large number of `Normal_Weight` as `Insufficient_Weight`.\n",
    "  - Also confuses between `Obesity_Type_I`, ` Overweight_Level_I`, and ` Overweight_Level_II`.\n",
    "\n",
    "- **Gradient Boost Confusion Matrix:**\n",
    "  - Much cleaner diagonals indicating better true positive rates.\n",
    "  - Very few misclassifications, even in closely related categories like overweight and obesity types.\n",
    "\n",
    "---\n",
    "\n",
    "#####  Conclusion:\n",
    "**Gradient Boosting** is clearly the better performing model based on overall accuracy, macro averaged metrics, and confusion matrix clarity. It is the recommended choice for final deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a2fa19",
   "metadata": {},
   "source": [
    "### Save to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108b833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename= 'Obesity.pkl'\n",
    "pickle.dump(gb_pipeline, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8dbb3c",
   "metadata": {},
   "source": [
    "- The Gradient Boosting model is saved using pickle\n",
    "- This allows the model to be reused for deployment in FastAPI and Streamlit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
